{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying this tutorial: https://www.natekratzer.com/posts/census_map/\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in file and mapping counties using WI as example\n",
    "county_shp = gpd.read_file('cb_2022_us_county_500k.zip')\n",
    "#wi_counties = county_shp[county_shp.STATE_NAME == 'Wisconsin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wi_counties.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load API key\n",
    "with open('census_api_key.txt') as key:\n",
    "    api_key=key.read().strip()\n",
    "\n",
    "#specify the data source by year and survey\n",
    "year = '2020'\n",
    "dsource = 'dec' #Decennial Census\n",
    "dname = 'dhc' #Demographic and Housing Characteristics\n",
    "base_url = f'https://api.census.gov/data/{year}/{dsource}/{dname}'\n",
    "\n",
    "#unique to this specific data request\n",
    "cols = 'NAME,P1_001N' #NAME of geography as well as the variables I want to pull\n",
    "geo = 'county:*' #county geography level\n",
    "state = 'state:*' #all states\n",
    "\n",
    "#add unique request features to the base_url\n",
    "data_url = f'{base_url}?get={cols}&for={geo}&in={state}&key={api_key}'\n",
    "\n",
    "#go get the data\n",
    "response = requests.get(data_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the data in json and format it into a dataframe\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = data[1:], columns = data[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns and convert population to numeric\n",
    "df.rename(columns = {'NAME': 'county_name', 'P1_001N': 'population', 'state': 'state_fips', 'county': 'county_fips'}, inplace=True)\n",
    "df['population'] = df['population'].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fips'] = df['state_fips'] + df['county_fips']\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = county_shp.STATE_NAME.values.tolist()\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a full US map\n",
    "usa_counties = county_shp[county_shp.STATE_NAME == states]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_counties.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct the fips code to match with the geography data\n",
    "usa_counties['fips'] = usa_counties['STATEFP'] + usa_counties['COUNTYFP']\n",
    "usa_counties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge in the new data\n",
    "#clean up the names to be a bit more presentable\n",
    "df_map = (usa_counties.merge(df, how = 'left', on = 'fips').rename(columns = {'NAMELSAD': 'County Name', 'population': 'County Population'}))\n",
    "\n",
    "df_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map.explore(\n",
    "    column ='County Population',\n",
    "    scheme=\"naturalbreaks\",  # use mapclassify's natural breaks scheme\n",
    "    legend=True,  # show legend\n",
    "    k=5,  # use 5 bins\n",
    "    tooltip = ['County Name', 'County Population'],\n",
    "    tiles = 'CartoDB positron', #fades into the background better than the default\n",
    "    legend_kwds=dict(colorbar=False, color=\"gray\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_counties = df_map[df_map['County Population'] > 5000000]\n",
    "biggest_counties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another approach I found, but did not work well bc of size of dataset and rate limits\n",
    "#pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note: rate limit became a problem; will need to filter down to CA data only before retrying\n",
    "#from geopy.geocoders import Nominatim\n",
    "\n",
    "#def get_zip_code(latitude, longitude): \n",
    "    #geolocator = Nominatim(user_agent=\"mafphd@icloud.com\")\n",
    "    #location = geolocator.reverse((latitude, longitude), exactly_one=True)\n",
    "    #address = location.raw['address']\n",
    "    #zip_code = address.get('postcode')\n",
    "    \n",
    "    #return zip_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "burn = pd.read_csv('Burn_Severity_Trends.csv')\n",
    "burn.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#burn['ZIP_CODE'] = burn.apply(lambda row: get_zip_code(row['LATITUDE'], row['LONGITUDE']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to identify counties by latitude/longitude coordinates using TIGER/Line Shapefile, 2019, nation, U.S., Current County and Equivalent National Shapefile from data.gov\n",
    "import geopandas as gpd\n",
    "\n",
    "# Load the shapefile\n",
    "gdf = gpd.read_file('tl_2019_us_county/tl_2019_us_county.shp') #file is too large to commit; download from https://catalog.data.gov/dataset/tiger-line-shapefile-2019-nation-u-s-current-county-and-equivalent-national-shapefile\n",
    "\n",
    "def get_county_name(latitude, longitude):\n",
    "    point = gpd.GeoDataFrame(geometry=gpd.points_from_xy([longitude], [latitude]))\n",
    "    county = gdf[gdf.contains(point.unary_union)]\n",
    "    return county['NAME'].iloc[0] if not county.empty else None\n",
    "\n",
    "# Example usage\n",
    "latitude = 37.7749  # Example latitude\n",
    "longitude = -122.4194  # Example longitude\n",
    "\n",
    "county_name = get_county_name(latitude, longitude)\n",
    "print(\"County Name:\", county_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn['County'] = burn.apply(lambda row: get_county_name(row['LATITUDE'], row['LONGITUDE']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn['County'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the missing counties to see if any are within approximate CA coordinates (from ChatGPT/verified on a map: min_lat = 32.5, max_lat = 37.0,min_lon = -125.0,max_lon = -114.0)\n",
    "missing_ctys = burn[burn['County'].isna()]\n",
    "missing_ctys_sorted = missing_ctys[['LATITUDE', 'LONGITUDE']]\n",
    "missing_ctys_sorted.sort_values(by='LATITUDE', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using BallTree nearest neighbors approach to fill in County based on closest known lat/long coordinates\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# Assuming 'burn' is your original DataFrame\n",
    "\n",
    "# Create a subset DataFrame with only rows that have County information\n",
    "known_counties = burn.dropna(subset=['County'])\n",
    "\n",
    "# Extract latitude and longitude for known counties\n",
    "known_coordinates = known_counties[['LATITUDE', 'LONGITUDE']].values\n",
    "\n",
    "# Build a BallTree for efficient nearest neighbor search\n",
    "tree = BallTree(known_coordinates, leaf_size=15)\n",
    "\n",
    "# Define a function to impute missing County values based on nearest neighbors\n",
    "def impute_county(row):\n",
    "    if pd.isna(row['County']):\n",
    "        distances, indices = tree.query([[row['LATITUDE'], row['LONGITUDE']]], k=1)\n",
    "        closest_county = known_counties.iloc[indices[0][0]]['County']\n",
    "        return closest_county\n",
    "    else:\n",
    "        return row['County']\n",
    "\n",
    "# Apply the impute_county function to fill in missing values\n",
    "burn['County'] = burn.apply(impute_county, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn['County'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking to see if I can get fips codes from the shapefile to join the two dataframes\n",
    "gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bringing FP codes to the burn dataframe for joining\n",
    "# Spatial join between burn DataFrame and shapefile\n",
    "gdf = gdf[['STATEFP', 'COUNTYFP', 'geometry']]  # Select only necessary columns\n",
    "burn_gdf = gpd.GeoDataFrame(burn, geometry=gpd.points_from_xy(burn.LONGITUDE, burn.LATITUDE), crs=gdf.crs)\n",
    "merged = gpd.sjoin(burn_gdf, gdf, how='left', op='within')\n",
    "\n",
    "# Extract STATEFP and COUNTYFP values\n",
    "burn['state_fips'] = merged['STATEFP']\n",
    "burn['county_fips'] = merged['COUNTYFP']\n",
    "\n",
    "# Create 'fips' column by concatenating 'state_fips' and 'county_fips'\n",
    "burn['fips'] = burn['state_fips'].astype(str) + burn['county_fips'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering down to California only for both dataframes\n",
    "Cali = df_map[df_map['STATE_NAME']=='California']\n",
    "Cali.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cali_burn = burn[burn['state_fips'] == '06']\n",
    "Cali_burn.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn_pop_merge = Cali.merge(Cali_burn, how = 'left', on = 'fips')\n",
    "burn_pop_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping extra columns\n",
    "burn_pop_merge.drop(['county_name', 'state_fips_y', 'county_fips_y', 'County'], axis=1, inplace=True)\n",
    "burn_pop_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extras = burn_pop_merge[burn_pop_merge['POST_ID'].isna()]\n",
    "print(extras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extras.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the two extra rows that got added, since they don't seem to have any info from the burn data\n",
    "burn_pop_merge = burn_pop_merge.dropna(subset=['POST_ID'])\n",
    "burn_pop_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick visualization of the results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Group by 'County Name' and get the count of POST_ID\n",
    "county_counts = burn_pop_merge.groupby('County Name')['POST_ID'].count()\n",
    "\n",
    "# Filter for counties with more than 5 POST_ID values\n",
    "county_counts_filtered = county_counts[county_counts > 5]\n",
    "\n",
    "# Merge with the original DataFrame to keep 'County Population'\n",
    "merged_df = burn_pop_merge.merge(county_counts_filtered, left_on='County Name', right_index=True, suffixes=('_original', '_count'))\n",
    "\n",
    "# Sort counties by County Population in descending order\n",
    "sorted_df = merged_df.sort_values(by='County Population', ascending=False)\n",
    "# Create the bar chart\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.barplot(x=sorted_df['County Name'], y=sorted_df['POST_ID_count'],color='firebrick')\n",
    "plt.title('Counties with > 5 fire records, Sorted by Population')\n",
    "plt.xlabel('County Name')\n",
    "plt.ylabel('Fires')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datathon-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
